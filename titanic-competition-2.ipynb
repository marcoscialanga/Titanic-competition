{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we will try to predict who survived the Titanic disaster. Let's import the necessary libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport matplotlib \nimport scipy as sp \nimport IPython\nfrom IPython import display \nimport sklearn \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the data now."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape, test_data.shape, train_data.count(), test_data.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice we have a mix of numerical and categorical data. We also have a lot of missing values, especially under the \"Cabin\" column (much more than half of the values are missing). Thus, we can drop it. Instead, we can use fillna() to fill in the missing values for the other columns. Furthermore, we will drop the columns \"Ticket\" (we won't attempt to get any information from this column) and \"Passenger ID\" (as it does not provide additional information on the passengers)."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Age'].fillna(dataset['Age'].mean(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n\npass\ncolumns_to_drop = [\"PassengerId\", \"Cabin\", \"Ticket\"]\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will build new columns that could help us with the prediction. We will also get the title of each person from the \"Name\" column, which is otherwise not useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data:\n    dataset[\"Family_size\"] = dataset [\"SibSp\"] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['Family_size'] > 1] = 0\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand = True)[1].str.split(\".\", expand = True)[0]\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    dataset['FareBin'] = pd.cut(dataset['Fare'].astype(int), 4)\n\nstat_min = 7\ntrain_title_names = (train_data['Title'].value_counts() < stat_min)\ntest_title_names = (test_data['Title'].value_counts() < stat_min)\n\ntrain_data['Title'] = train_data['Title'].apply(lambda x: 'Misc' if train_title_names.loc[x] == True else x)\ntest_data['Title'] = test_data['Title'].apply(lambda x: 'Misc' if test_title_names.loc[x] == True else x)\n\n\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = LabelEncoder()\n\nfor dataset in data: \n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\npass\n\n#train_y = ['Survived']\n#train_X = ['Sex', 'Pclass', 'Embarked_Code', 'Title_Code', 'SibSp', 'Parch', 'AgeBin_Code', 'FareBin_Code', 'IsAlone']\n#test_X = ['Sex', 'Pclass', 'Embarked_Code', 'Title_Code', 'SibSp', 'Parch', 'AgeBin_Code', 'FareBin_Code', 'IsAlone']\n\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Target = ['Survived']\ndata1_x = ['Family_size', 'Age', 'Fare', 'Sex','Pclass', 'Embarked', 'Title', 'SibSp', 'Parch', 'IsAlone']\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare']\ndata1_xy =  Target + data1_x\n\n\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(train_data[data1_x])\ndata2_dummy = pd.get_dummies(test_data[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\n\n\ndata1_dummy.insert(18, 'Survived', train_data['Survived'])\ndata1_dummy.insert(0, 'PassengerId', train_data['PassengerId'])\ndata2_dummy.insert(0, 'PassengerId', test_data['PassengerId'])\ntrain_data = data1_dummy\ntest_data = data2_dummy\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's build our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"Titanic_features = [\"Age\", \"Fare\", \"Pclass\", \"IsAlone\", \"SibSp\", \"Parch\",\"Sex_female\",\"Sex_male\",\"Embarked_C\",\"Embarked_Q\",\"Embarked_S\",\"Title_Master\", \"Title_Misc\",\"Title_Miss\",\"Title_Mr\",\"Title_Mrs\"]\nTitanic_prediction_target = ['Survived']\n\ntrain_X = train_data[Titanic_features]\ntest_X = test_data[Titanic_features]\ntrain_y = train_data[Titanic_prediction_target]\n\ntest_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find the best parameters for our Random Forest Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [int(x) for x in np.linspace(start = 0, stop = 2000, num = 11)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n                               n_iter = 100, cv = 3, verbose = 2, \n                               random_state = 42, n_jobs = -1)\nrf_random.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, using these parameters, we can build our Classification Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_model = RandomForestClassifier(min_samples_split = 40, \n                             max_leaf_nodes = 15, \n                             n_estimators = 40, \n                             max_depth = 5,\n                             min_samples_leaf = 3,\n                             max_features = 'sqrt')\n\nmy_model.fit(train_X, train_y.values.ravel())\nmy_model.predict(train_X)\nmy_model_predictions = my_model.predict(test_X)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': my_model_predictions})\nprint(output)\noutput.to_csv(\"my_submission.csv\", index=False)\nprint(\"You submitted the file\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We achieved a score of about %78.23. "}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}